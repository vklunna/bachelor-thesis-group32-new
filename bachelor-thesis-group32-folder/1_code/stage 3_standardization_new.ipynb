{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1ad6e-eead-4240-b700-2695c91bcae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb8446da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "from unidecode import unidecode  # for transliteration like é → e\n",
    "\n",
    "# Regex patterns\n",
    "combined_code_pattern = re.compile(\n",
    "    r'^(ESRS\\s*\\d?\\s*[A-Z]{2,4}\\s*[-.]\\d+(?:\\.\\d+)?[a-zA-Z]?)\\s*[-–—]\\s*(.*)',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "esrs_code_pattern = re.compile(\n",
    "    r'^(ESRS\\s*\\d?\\s*)?[A-Z]{2,4}\\s*[-.]\\d+(?:\\.\\d+)?[a-zA-Z]?$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "extended_code_pattern = re.compile(\n",
    "    r'^(ESRS\\s*\\d?\\s*)?[A-Z]{2,4}\\s*[-.]\\d+(?:\\.\\d+)?[a-zA-Z]?$',\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def extract_code_chunks(text):\n",
    "    \"\"\"\n",
    "    Extract ESRS-like codes from text.\n",
    "    Handles cases like:\n",
    "    - E1-1, E1 -1\n",
    "    - GOV-2, SBM -1\n",
    "    - ESRS 2 E1-3, ESRS2 G1 -1\n",
    "    - E2-SBM-1\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "\n",
    "    text = re.sub(r'[‐‒–—―−－⁻]', '-', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    # ✅ Limit to allowed ESRS prefixes\n",
    "    allowed_prefixes = r'(?:E|S|G|GOV|SBM|IRO|BP)'\n",
    "    code_pattern = rf'((?:ESRS\\s*\\d?\\s*)?(?:{allowed_prefixes})(?:[-.]?\\d+)*(?:[-.]\\d+(?:\\.\\d+)?[a-zA-Z]?)*)'\n",
    "\n",
    "    pattern = re.compile(\n",
    "        rf'(?:^|\\|\\s*|\\s+)'   # Start of line or separator\n",
    "        rf'{code_pattern}'    # The actual code\n",
    "        rf'(?=\\s|\\||$)',      # End of code\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    matches = []\n",
    "    last_end = 0\n",
    "    for match in pattern.finditer(text):\n",
    "        full_code = match.group(1).strip()\n",
    "        full_code = re.sub(r'\\s*-\\s*', '-', full_code)  # Normalize spacing around dashes\n",
    "        last_end = match.end()\n",
    "        matches.append((full_code, ''))\n",
    "\n",
    "    if matches:\n",
    "        trailing = text[last_end:].strip().lstrip('|').strip()\n",
    "        matches[-1] = (matches[-1][0], trailing)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def clean_extracted_content(df):\n",
    "    \"\"\"Clean DataFrame after code extraction\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return df\n",
    "        \n",
    "    df = df.copy()\n",
    "    for idx, row in df.iterrows():\n",
    "        for col in ['Name', 'Description']:\n",
    "            text = str(row[col])\n",
    "            for code, _ in extract_code_chunks(text):\n",
    "                text = re.sub(rf'\\s*{re.escape(code)}\\s*[\\|]*\\s*', ' ', text)\n",
    "            df.at[idx, col] = text.strip()\n",
    "    return df\n",
    "\n",
    "def split_rows_on_pipe_with_code(df):\n",
    "    \"\"\"Split rows with pipe-separated codes\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        parts = [p.strip() for p in str(row['Name']).split('|') if p.strip()]\n",
    "        for part in parts:\n",
    "            new_row = row.copy()\n",
    "            codes = extract_code_chunks(part)\n",
    "            if codes:\n",
    "                for code, remaining in codes:\n",
    "                    new_row['Code'] = code\n",
    "                    new_row['Name'] = remaining\n",
    "                    expanded_rows.append(new_row.copy())\n",
    "            else:\n",
    "                new_row['Name'] = part\n",
    "                expanded_rows.append(new_row.copy())\n",
    "    return pd.DataFrame(expanded_rows)\n",
    "\n",
    "\n",
    "def process_dataframe(input_df):\n",
    "    \"\"\"Main processing pipeline\"\"\"\n",
    "    if not isinstance(input_df, pd.DataFrame):\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # Clean content first\n",
    "    df_clean = clean_extracted_content(input_df)\n",
    "    \n",
    "    # Process Description column\n",
    "    desc_expanded = []\n",
    "    for _, row in df_clean.iterrows():\n",
    "        desc_chunks = extract_code_chunks(row['Description'])\n",
    "        if not desc_chunks:\n",
    "            desc_expanded.append(row.to_dict())\n",
    "        else:\n",
    "            for code, context in desc_chunks:\n",
    "                new_row = row.copy()\n",
    "                new_row['Code'] = code\n",
    "                new_row['Description'] = context\n",
    "                desc_expanded.append(new_row)\n",
    "    \n",
    "    # Process Name column\n",
    "    name_df = split_rows_on_pipe_with_code(pd.DataFrame(desc_expanded))\n",
    "    return name_df\n",
    "    \n",
    "# Helper: split \"code name\" if fused (fallback)\n",
    "def split_code_and_name(text):\n",
    "    codes = extract_code_chunks(text)\n",
    "    if codes:\n",
    "        code = codes[0][0]\n",
    "        remaining = text.replace(code, '').strip()\n",
    "        return code, remaining\n",
    "    return \"\", text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def split_rows_on_any_embedded_esrs_code(df):\n",
    "    \"\"\"Split rows where Name or Description contain multiple ESRS codes (with or without pipes).\"\"\"\n",
    "    expanded_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        original_code = row.get(\"Code\", \"\")\n",
    "        page_range = row.get(\"Page Range\", \"\")\n",
    "        relevant_pages = row.get(\"Relevant Pages\", \"\")\n",
    "        name_text = str(row.get(\"Name\", \"\") or \"\")\n",
    "        description_text = str(row.get(\"Description\", \"\") or \"\")\n",
    "\n",
    "        # Extract chunks with codes from both fields\n",
    "        name_chunks = extract_code_chunks(name_text)\n",
    "        desc_chunks = extract_code_chunks(description_text)\n",
    "\n",
    "        # If no multiple codes → keep row as is\n",
    "        if len(name_chunks) + len(desc_chunks) <= 1:\n",
    "            expanded_rows.append({\n",
    "                \"Code\": original_code,\n",
    "                \"Name\": name_text,\n",
    "                \"Page Range\": page_range,\n",
    "                \"Description\": description_text,\n",
    "                \"Relevant Pages\": relevant_pages\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Expand Name-based codes\n",
    "        for code, _ in name_chunks:\n",
    "            cleaned_name = re.sub(re.escape(code), \"\", name_text).replace(\"|\", \" \").strip()\n",
    "            expanded_rows.append({\n",
    "                \"Code\": code,\n",
    "                \"Name\": cleaned_name,\n",
    "                \"Page Range\": page_range,\n",
    "                \"Description\": \"\",\n",
    "                \"Relevant Pages\": relevant_pages\n",
    "            })\n",
    "\n",
    "        # Expand Description-based codes\n",
    "        for code, _ in desc_chunks:\n",
    "            cleaned_desc = re.sub(re.escape(code), \"\", description_text).replace(\"|\", \" \").strip()\n",
    "            expanded_rows.append({\n",
    "                \"Code\": code,\n",
    "                \"Name\": \"\",\n",
    "                \"Page Range\": page_range,\n",
    "                \"Description\": cleaned_desc,\n",
    "                \"Relevant Pages\": relevant_pages\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(expanded_rows, columns=[\"Code\", \"Name\", \"Page Range\", \"Description\", \"Relevant Pages\"])\n",
    "\n",
    "def split_rows_by_pipe_alignment_with_codes(df):\n",
    "    \"\"\"Split rows where multiple ESRS codes exist in 'Name' and align with 'Description' using pipe index.\"\"\"\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        name_text = str(row.get(\"Name\", \"\") or \"\")\n",
    "        desc_text = str(row.get(\"Description\", \"\") or \"\")\n",
    "        code = row.get(\"Code\", \"\")\n",
    "        page_range = row.get(\"Page Range\", \"\")\n",
    "        relevant_pages = row.get(\"Relevant Pages\", \"\")\n",
    "\n",
    "        # Split by pipe and strip whitespace\n",
    "        name_parts = [n.strip() for n in re.split(r'\\s*\\|\\s*', name_text)]\n",
    "        desc_parts = [d.strip() for d in re.split(r'\\s*\\|\\s*', desc_text)]\n",
    "\n",
    "        # Extract code chunks from name parts\n",
    "        for i, name_chunk in enumerate(name_parts):\n",
    "            matched_codes = extract_code_chunks(name_chunk)\n",
    "\n",
    "            # Try to get matching description part\n",
    "            description_chunk = desc_parts[i] if i < len(desc_parts) else \"\"\n",
    "\n",
    "            if matched_codes:\n",
    "                for code_found, _ in matched_codes:\n",
    "                    # Remove code from name_chunk with surrounding whitespace and optional \"ESRS\"\n",
    "                    name_cleaned = re.sub(\n",
    "                        rf'\\b(ESRS\\s*)?{re.escape(code_found)}\\b', '', name_chunk, flags=re.IGNORECASE\n",
    "                    ).strip()\n",
    "                    new_rows.append({\n",
    "                        \"Code\": code_found.strip(),\n",
    "                        \"Name\": name_cleaned,\n",
    "                        \"Page Range\": page_range,\n",
    "                        \"Description\": description_chunk,\n",
    "                        \"Relevant Pages\": relevant_pages\n",
    "                    })\n",
    "            else:\n",
    "                new_rows.append({\n",
    "                    \"Code\": code.strip(),\n",
    "                    \"Name\": name_chunk,\n",
    "                    \"Page Range\": page_range,\n",
    "                    \"Description\": description_chunk,\n",
    "                    \"Relevant Pages\": relevant_pages\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(new_rows, columns=[\"Code\", \"Name\", \"Page Range\", \"Description\", \"Relevant Pages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a06b4dc-d2c1-4a05-849f-b5134a6a3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Function: preserve content + separate valid codes + filter out header/title rows\n",
    "def detect_and_correct_misplaced_codes(df):\n",
    "    new_rows = []\n",
    "\n",
    "    code_pattern = re.compile(\n",
    "        r'\\b(?:ESRS\\s*)?((?:GOV|[EGS])\\d*(?:[-.]\\d+)+(?:[a-zA-Z])?)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        code, name, page_range, description = row[\"Code\"], row[\"Name\"], row[\"Page Range\"], row[\"Description\"]\n",
    "\n",
    "        if not code or pd.isna(code) or str(code).strip() == \"\":\n",
    "            matches = code_pattern.findall(name)\n",
    "            if matches:\n",
    "                detected_code = matches[0].strip()\n",
    "                if not re.fullmatch(r'\\d+(\\.\\d+)?', detected_code):  # avoid floats like 1.1\n",
    "                    code = detected_code\n",
    "                    name = name.replace(detected_code, '').strip()\n",
    "\n",
    "        new_rows.append([code, name, page_range, description])\n",
    "\n",
    "    return pd.DataFrame(new_rows, columns=[\"Code\", \"Name\", \"Page Range\", \"Description\"])\n",
    "\n",
    "def clean_relevant_pages_from_range(row):\n",
    "    rel_pages = {int(p) for p in str(row[\"Relevant Pages\"]).split(\",\") if p.strip().isdigit()}\n",
    "    page_range_nums = {int(p) for p in re.findall(r'\\d{2,4}', str(row[\"Page Range\"]))}\n",
    "\n",
    "    cleaned = sorted(rel_pages - page_range_nums)\n",
    "    return \",\".join(str(p) for p in cleaned)\n",
    "\n",
    "\n",
    "def extract_relevant_pages(description: str, name: str = \"\", page_range: str = \"\") -> str:\n",
    "    import re\n",
    "\n",
    "    def extract_from_text(text):\n",
    "        text = text.lower()\n",
    "        pages = set()\n",
    "        parts = re.split(r'\\s*\\|\\s*', text)\n",
    "\n",
    "        for part in parts:\n",
    "            if re.search(r'\\b(paragraph|section)\\s+\\d+', part):\n",
    "                continue\n",
    "\n",
    "            part = re.sub(r'\\b(?:\\d{1,2}[-/.]){2}\\d{2,4}\\b', '', part)\n",
    "            part = re.sub(r'\\b(january|february|march|april|may|june|july|august|'\n",
    "                          r'september|october|november|december)\\b', '', part)\n",
    "            part = re.sub(r'\\b[a-z]?\\d+(?:\\.\\d+){1,3}[a-z]?\\b', '', part)\n",
    "\n",
    "            for start, end in re.findall(r'(?<!\\d)(\\d{2,4})\\s*[-–—]\\s*(\\d{2,4})(?!\\d)', part):\n",
    "                s, e = int(start), int(end)\n",
    "                if s < e and 10 <= s <= 1500 and 10 <= e <= 1500:\n",
    "                    pages.update(range(s, e + 1))\n",
    "\n",
    "            for match in re.findall(r'\\b(\\d{2,4})\\b', part):\n",
    "                n = int(match)\n",
    "                if 10 <= n <= 1500:\n",
    "                    pages.add(n)\n",
    "\n",
    "            if re.search(r'(\\d{2,4})\\s*\\|$', part):\n",
    "                n = int(re.search(r'(\\d{2,4})\\s*\\|$', part).group(1))\n",
    "                if 10 <= n <= 1500:\n",
    "                    pages.add(n)\n",
    "\n",
    "            tokens = part.strip().split()\n",
    "            if tokens and tokens[-1].isdigit():\n",
    "                n = int(tokens[-1])\n",
    "                if 10 <= n <= 1500:\n",
    "                    pages.add(n)\n",
    "\n",
    "        return pages\n",
    "\n",
    "    # ✅ Extract numbers from name and description\n",
    "    name_pages = extract_from_text(name if isinstance(name, str) else \"\")\n",
    "    desc_pages = extract_from_text(description if isinstance(description, str) else \"\")\n",
    "    all_pages = name_pages.union(desc_pages)\n",
    "\n",
    "    # ✅ Parse page_range string → set of ints\n",
    "    excluded = set()\n",
    "    if isinstance(page_range, str):\n",
    "        for val in re.findall(r'\\d{2,4}', page_range):\n",
    "            try:\n",
    "                n = int(val)\n",
    "                if 10 <= n <= 1500:\n",
    "                    excluded.add(n)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # ✅ Remove duplicates and page range values\n",
    "    final_pages = sorted(p for p in all_pages if p not in excluded)\n",
    "    return \",\".join(str(p) for p in final_pages)\n",
    "    \n",
    "\n",
    "def clean_extracted_content(df):\n",
    "    \"\"\"Clean DataFrame after code extraction\"\"\"\n",
    "    if not isinstance(df, pd.DataFrame) or df.empty:\n",
    "        return df\n",
    "        \n",
    "    df = df.copy()\n",
    "    for idx, row in df.iterrows():\n",
    "        for col in ['Name', 'Description']:\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            text = str(row[col]) if pd.notna(row[col]) else \"\"\n",
    "            try:\n",
    "                for code, _ in extract_code_chunks(text):\n",
    "                    text = re.sub(rf'(\\|?\\s*{re.escape(code)}\\s*\\|?)', ' ', text)\n",
    "                text = re.sub(r'\\s+', ' ', text).strip()\n",
    "                df.at[idx, col] = text\n",
    "            except Exception as e:\n",
    "                print(f\"Error cleaning row {idx} column {col}: {str(e)}\")\n",
    "                df.at[idx, col] = text.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def full_content_preserving_standardize(filepath):\n",
    "    \"\"\"Standardize a single CSV file with error handling\"\"\"\n",
    "    try:\n",
    "        input_df = pd.read_csv(filepath)\n",
    "        input_df.columns = input_df.columns.str.strip()  # ✅ Strip column names\n",
    "\n",
    "# ✅ Extract page number from filename or folder\n",
    "        file_path = Path(filepath)\n",
    "        page_match = re.search(r'page[_\\-]?0*([1-9]\\d{0,3})', str(file_path.name))  # Try from filename\n",
    "        if not page_match:\n",
    "            page_match = re.search(r'page[_\\-]?0*([1-9]\\d{0,3})', str(file_path.parent.name))  # Fallback to folder\n",
    "\n",
    "        page_str = page_match.group(1) if page_match else ''\n",
    "\n",
    "# ✅ Add or overwrite Page Range and Relevant Pages columns\n",
    "        input_df[\"Page Range\"] = page_str\n",
    "        input_df[\"Relevant Pages\"] = page_str\n",
    "\n",
    "# Add Page Range column if missing\n",
    "        if \"Page Range\" not in input_df.columns:\n",
    "            input_df[\"Page Range\"] = page_str\n",
    "    except Exception as e:\n",
    "        return None, f\"Error reading {filepath}: {e}\"\n",
    "\n",
    "    # Initial cleaning and standardization\n",
    "    input_df.dropna(axis=1, how='all', inplace=True)\n",
    "    for col in input_df.select_dtypes(include='object').columns:\n",
    "        input_df[col] = input_df[col].fillna('')\n",
    "\n",
    "    standardized_rows = []\n",
    "    for _, row in input_df.iterrows():\n",
    "        cells = [str(cell).strip() for cell in row if str(cell).strip()]\n",
    "        if not cells:\n",
    "            continue\n",
    "\n",
    "        code = name = description = \"\"\n",
    "        page_range = row['Page Range'] if 'Page Range' in row else ''\n",
    "        \n",
    "        # Pattern 1: Combined \"ESRS X-Y — Title\"\n",
    "        match_combined = combined_code_pattern.match(cells[0])\n",
    "        if match_combined:\n",
    "            code = match_combined.group(1).strip()\n",
    "            name = match_combined.group(2).strip()\n",
    "            description = \" \".join(cells[1:]) if len(cells) > 1 else \"\"\n",
    "        \n",
    "        # Pattern 2: Code in first cell\n",
    "        elif esrs_code_pattern.match(cells[0]):\n",
    "            code = cells[0]\n",
    "            name = cells[1] if len(cells) > 1 else \"\"\n",
    "            description = \" \".join(cells[2:]) if len(cells) > 2 else \"\"\n",
    "        \n",
    "        # Pattern 3: Extended\n",
    "        elif extended_code_pattern.match(cells[0]):\n",
    "            code = cells[0]\n",
    "            name = cells[1] if len(cells) > 1 else \"\"\n",
    "            description = \" \".join(cells[2:]) if len(cells) > 2 else \"\"\n",
    "        \n",
    "        # Fallback\n",
    "        else:\n",
    "            code, name = split_code_and_name(cells[0])\n",
    "            description = \" \".join(cells[1:]) if len(cells) > 1 else \"\"\n",
    "\n",
    "        standardized_rows.append([code, name, page_range, description])\n",
    "\n",
    "    try:\n",
    "        df_cleaned = pd.DataFrame(\n",
    "            standardized_rows,\n",
    "            columns=[\"Code\", \"Name\", \"Page Range\", \"Description\"]\n",
    "        )\n",
    "        \n",
    "        # Apply processing pipeline\n",
    "        processed_df = process_dataframe(df_cleaned)\n",
    "        processed_df = detect_and_correct_misplaced_codes(processed_df)\n",
    "\n",
    "# 🔁 NEW STEP: First split by aligned pipes with code pairing\n",
    "        processed_df = split_rows_by_pipe_alignment_with_codes(processed_df)\n",
    "\n",
    "# 🔁 THEN: Fallback to embedded code splitting if needed\n",
    "        processed_df = split_rows_on_any_embedded_esrs_code(processed_df)\n",
    "\n",
    "        processed_df[\"Relevant Pages\"] = page_str\n",
    "        \n",
    "        # Final grouping\n",
    "        final_df = processed_df.groupby('Code').agg({\n",
    "            'Name': lambda x: \" | \".join(x.dropna().unique()),\n",
    "            'Page Range': 'first',\n",
    "            'Description': lambda x: \" | \".join(x.dropna().unique()),\n",
    "            'Relevant Pages': lambda x: \",\".join(sorted(\n",
    "                {p for p in \",\".join(x).split(\",\") if p.strip()},\n",
    "                key=lambda p: int(p) if p.isdigit() else float('inf')\n",
    "            ))\n",
    "        }).reset_index()\n",
    "        \n",
    "        return final_df, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error processing {filepath}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aede91-9fe7-4ac2-92f0-ba11ccb96221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_section_reference(name: str, description: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts section/paragraph references from name & description with logic:\n",
    "    - Allows combinations starting with a number (e.g., 4.2a, 3-1, 2.1.1) but never starting with 0\n",
    "    - Standalone numbers (e.g., '22') are only extracted if preceded by 'section', 'paragraph', or 'chapter'\n",
    "    - Captures trailing titles only if the first word starts with a capital letter\n",
    "    - Discards anything starting with a letter (e.g., 'E1-4')\n",
    "    - Skips false matches like numbers inside 'ESRS-2'\n",
    "    - Skips anything ending in .0, -0, or (0)\n",
    "    - Skips combinations that look like page ranges (e.g. 127-128, 101.0) if values fall in page range range\n",
    "    - Ensures sections are not captured if followed directly by likely page numbers\n",
    "    - Final results are comma-separated (not pipe)\n",
    "    \"\"\"\n",
    "\n",
    "    import re\n",
    "\n",
    "    def find_phrases(text):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "\n",
    "        text = re.sub(r'[‐‒–—]', '-', text).strip()\n",
    "\n",
    "        pattern = re.compile(\n",
    "            r'(?<![\\w-])'                                   # Not part of another word or dash before\n",
    "            r'(?:(?P<prefix>paragraph|section|chapter)\\s+)?'  # Optional prefix: paragraph, section, or chapter\n",
    "            r'(?P<ref>[1-9]\\d{0,3}(?:[-.]\\d+){0,6}(?:\\([a-z]\\))?)'  # Must start with non-zero digit\n",
    "            r'(?:\\s+(?P<tail>[A-Z][^\\d\\|\\n\\r]*))?'      # Optional phrase (must start with capital)\n",
    "            r'(?=\\s+(?!\\d{1,4}(?![.-]))|\\||\\n|$)',        # Stop unless followed by clear page number\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        matches = []\n",
    "        for match in pattern.finditer(text):\n",
    "            prefix = match.group(\"prefix\") or \"\"\n",
    "            ref = match.group(\"ref\").strip()\n",
    "            tail = (match.group(\"tail\") or '').strip()\n",
    "\n",
    "            if not ref[0].isdigit() or ref[0] == '0':\n",
    "                continue\n",
    "\n",
    "            if re.fullmatch(r'\\d{1,4}', ref) and not prefix:\n",
    "                continue\n",
    "\n",
    "            if ref.isdigit() and int(ref) > 1000:\n",
    "                continue\n",
    "\n",
    "            if re.search(r'(\\.0|-0|\\(0\\))$', ref):\n",
    "                continue\n",
    "\n",
    "            if re.fullmatch(r'\\d{2,4}[-.]\\d{1,4}', ref):\n",
    "                try:\n",
    "                    parts = [int(x) for x in re.split(r'[-.]', ref)]\n",
    "                    if all(10 <= x <= 1500 for x in parts):\n",
    "                        continue\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            combined = f\"{prefix} {ref}\".strip()\n",
    "            if tail:\n",
    "                combined += f\" {tail}\"\n",
    "            matches.append(combined.strip())\n",
    "\n",
    "        return matches\n",
    "\n",
    "    refs = find_phrases(name) + find_phrases(description)\n",
    "    return \", \".join(dict.fromkeys(refs))  # unique & comma-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9db1a1cd-3b95-421a-bda0-8bc899ff9c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Volkswagen_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/RFA_ELO_2024_EN-1_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Commerzbank_Group_Annual_Report_2024_standardized_full.csv (rows: 48)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/upm-annual-report-2024_standardized_full.csv (rows: 15)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Carlsberg Group_2024 Annual Report_standardized_full.csv (rows: 44)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/brenntag_annual-report-2024_en_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Amorim_RC24_EN_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Cenergy-Annual_Report_2024_standardized_full.csv (rows: 26)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Annual-Report-HoldCo-2024-Web_standardized_full.csv (rows: 6)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/DHL-Group-2024-Annual-Report_standardized_full.csv (rows: 54)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/annual-report-2024-data_standardized_full.csv (rows: 73)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Orsted Annual Report 2024_standardized_full.csv (rows: 11)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/annual-report-2024-equinor-compressed_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/01UK_total_HR_standardized_full.csv (rows: 18)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/24-VUB-Vyrocna-sprava-ENG_standardized_full.csv (rows: 57)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/consolidated-annual-report-endesa-2024_standardized_full.csv (rows: 2)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/sopra_steria_urd_2024_en_opti_standardized_full.csv (rows: 11)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/ferrovial-integrated-annual-report-2024-1_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/ad_annual-report_2024_interactive_standardized_full.csv (rows: 31)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/f3bb40f8-e502-409f-b3b3-a61e2954531b_standardized_full.csv (rows: 33)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/adidas-ar24_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2024 ING Groep NV annual report_standardized_full.csv (rows: 20)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2025-04-23-Eramet-URD-2024-EN_standardized_full.csv (rows: 17)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/EXOR 2024 Annual Report_standardized_full.csv (rows: 38)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Wolters_Kluwer_2024_Annual_Report_standardized_full.csv (rows: 12)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/FiskarsGroup_Annual_Report_2024_standardized_full.csv (rows: 12)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/eri_euroapi2024_urd_en_basse-definition_31march_19h28_standardized_full.csv (rows: 2)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Evonik_Financial_and_Sustainability_Report_2024_standardized_full.csv (rows: 13)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/EUR_URD2024_MEL__standardized_full.csv (rows: 15)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/allianz-group-annual-report-2024_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Brunel-Annual-Report-2024-pdf_standardized_full.csv (rows: 26)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Informe_de_Gestion_Consolidado_2024_ENG_standardized_full.csv (rows: 5)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Basic-Fit Annual_Report_2024_Webversion_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/CNP-Assurances-SA-and-subsidiaries-2024-URD_standardized_full.csv (rows: 44)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/6-2024-KPN-Integrated-Annual-Report_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/WithSecure_Annual_Report_2024_standardized_full.csv (rows: 28)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/management-board-report-on-cd-projekt-group-activities-in-2024_standardized_full.csv (rows: 17)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/TomTom_Annual Report 2024 (PDF Version)_standardized_full.csv (rows: 13)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/airbus_report_of_the_board_of_directors_2024_standardized_full.csv (rows: 67)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Bureau-Veritas_Universal_Registration_Document_2024_16.3_Mo_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/vattenfall-annual-and-sustainability-report-2024_standardized_full.csv (rows: 2)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/entire-full-report-basf-ar24_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Valeo_2024-Universal-registration-document_standardized_full.csv (rows: 17)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Austrian Post Annual Report 2024_standardized_full.csv (rows: 20)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/FLS-annual-report-2024_standardized_full.csv (rows: 11)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/CLARIANE_DEU_2024_UK_standardized_full.csv (rows: 12)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/vinci-2024-universal-registration-document_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/verbund-integrated-annual-report-2024-englisch-final-1_standardized_full.csv (rows: 89)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2024 Adyen Annual Report_standardized_full.csv (rows: 15)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Zabka-Group-2024-AR-Full-year_standardized_full.csv (rows: 26)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/annual_report_2024_bnp_paribas_fortis_en_standardized_full.csv (rows: 50)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/finnair-financial-information-2024_standardized_full.csv (rows: 19)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/AR2024_FINAL_en_standardized_full.csv (rows: 13)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Corbion_annual_report_2024_standardized_full.csv (rows: 22)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Randstad_Annual_Report_2024_0_standardized_full.csv (rows: 7)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/bayer-annual-report-2024_standardized_full.csv (rows: 66)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2024-Acerinox-Group-Consolidated-Management-Report_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/management_board_report_on_activity_of_enea_and_enea_group_standardized_full.csv (rows: 77)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Elisa_AnnualReport_2024_standardized_full.csv (rows: 55)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Covestro_2024_GB_EN_standardized_full.csv (rows: 16)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/FMO Annual Report 2024_complete_LR_standardized_full.csv (rows: 18)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Stellantis-NV-20241231-Annual-Report_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/ABN_AMRO___Integrated_Annual_Report_2024_standardized_full.csv (rows: 9)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/41bba0a0-6f60-485f-9141-a50ef8acee26_standardized_full.csv (rows: 21)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Annual Report 2024 - for print_standardized_full.csv (rows: 9)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/UniCredit_standardized_full.csv (rows: 23)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Capgemini_-_2025-03-21_-_2024_Universal_Registration_Document_standardized_full.csv (rows: 26)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/fingrid_oyj_annual_report_2024_standardized_full.csv (rows: 56)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Terna_2024_Annual_Report_8dd871205a435e0_standardized_full.csv (rows: 67)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/bn_annual_report_2024_standardized_full.csv (rows: 8)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/cchbc-iar-2024.pdf.downloadasset_standardized_full.csv (rows: 40)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/BMW-Group-Report-2024-en_standardized_full.csv (rows: 22)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/bilancio integrato-20250404-ENG (2)_standardized_full.csv (rows: 35)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/CH_2024 Annual Report_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/FA_0124_Geschaeftsbericht_EN_RZ_250325_FINAL_standardized_full.csv (rows: 1)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2024 Financial Report_Volume1_Management, Sustainability and Governance_internet_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Annual Report 2024_Zalando SE_EN_250503_s_standardized_full.csv (rows: 17)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2.2 BoD Report 2024 EN -14.03.2025_FINAL_standardized_full.csv (rows: 22)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/f-secure-annual-report-2024_standardized_full.csv (rows: 9)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/f5937bce-c119-4d4d-91fd-9306c8d2c744_standardized_full.csv (rows: 40)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Veolia_2024_URD_standardized_full.csv (rows: 91)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2025-03-20-rwe-annual-report-2024_standardized_full.csv (rows: 14)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/2024 FBD HOLDINGS ANNUAL REPORT 2024-compressed_standardized_full.csv (rows: 20)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/f10e19e9-3289-43e1-85cc-5449908cf864_standardized_full.csv (rows: 55)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/ESSILORLUXOTTICA_URD-US-2024_MEL_standardized_full.csv (rows: 13)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Etteplan_standardized_full.csv (rows: 26)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/Vestas Annual Report 2024_standardized_full.csv (rows: 13)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/PhilipsFullAnnualReport2024-English_standardized_full.csv (rows: 42)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/fy-2024-download-sag-annual-report-en-data_standardized_full.csv (rows: 24)\n",
      "✅ Saved updated: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company/OMV Combined Annual Report 2024_standardized_full.csv (rows: 14)\n",
      "📂 Output Directory: /Users/valeriiaklynna/Documents/GitHub/bachelor-thesis-group32-new/bachelor-thesis-group32-folder/2_output/standardized_merged_by_company\n",
      "\n",
      "⚠️ Errors (if any):\n",
      "\n",
      "✅ Preview of Last Merged Company Table:\n",
      "        Code                                               Name  \\\n",
      "0             Shareholders Directors’ Report | and metrics r...   \n",
      "1  GOV-1.22c  [ESRS-2--i, 22c-ii, 22c-iii] This independent ...   \n",
      "2  GOV-1.22d  [ESRS-2-] OMV’s Enterprise-Wide Risk Managemen...   \n",
      "3  GOV-2.26a  [ESRS-2-, 26b] OMV Group Sustainability is the...   \n",
      "4  GOV-2.26c  [ESRS-2-] The list of all material impacts, ri...   \n",
      "5      GOV-3  Integration of Sustainability-Related Performa...   \n",
      "6  GOV-3.29a  [ESRS-2-] [E1-GOV-3.13] The Supervisory Board ...   \n",
      "7  GOV-3.29b  [ESRS-2--29d] [E1-GOV-3.13] The Remuneration P...   \n",
      "8      GOV-5         Risk Management and Internal Controls over   \n",
      "9  GOV-5.36a  [ESRS-2-] OMV has developed a robust internal ...   \n",
      "\n",
      "                                         Description Page Range  \\\n",
      "0  Governance Financial Statements Further Inform...        110   \n",
      "1                                        nan 110 110        110   \n",
      "2                                        nan 110 110        110   \n",
      "3                                        nan 110 110        110   \n",
      "4                                        nan 110 110        110   \n",
      "5                                        nan 111 111        111   \n",
      "6                                        nan 111 111        111   \n",
      "7                                        nan 111 111        111   \n",
      "8                                        nan 113 113        113   \n",
      "9                                        nan 113 113        113   \n",
      "\n",
      "  Relevant Pages Section Reference  \n",
      "0    10,15,20,30                    \n",
      "1                                   \n",
      "2                                   \n",
      "3                                   \n",
      "4                                   \n",
      "5                                   \n",
      "6                                   \n",
      "7                                   \n",
      "8                                   \n",
      "9                                   \n"
     ]
    }
   ],
   "source": [
    "REPO_ROOT = Path().resolve().parent  # Adjust path as needed\n",
    "filtered_tables_dir = REPO_ROOT / \"2_output\" / \"filtered_tables\"\n",
    "merged_output_dir = REPO_ROOT / \"2_output\" / \"standardized_merged_by_company\"\n",
    "os.makedirs(merged_output_dir, exist_ok=True)\n",
    "\n",
    "company_groups = defaultdict(list)\n",
    "company_error_log = {}\n",
    "last_merged_df = None\n",
    "\n",
    "# Step 1: Locate company CSVs, even inside page_### subfolders\n",
    "for root, dirs, files in os.walk(str(filtered_tables_dir)):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            path_parts = Path(root).parts\n",
    "            if 'filtered_tables' in path_parts:\n",
    "                idx = path_parts.index('filtered_tables')\n",
    "                if len(path_parts) > idx + 1:\n",
    "                    company = path_parts[idx + 1]\n",
    "                else:\n",
    "                    company = Path(root).name\n",
    "            else:\n",
    "                company = Path(root).name\n",
    "\n",
    "            company_groups[company].append(os.path.join(root, file))\n",
    "\n",
    "# Step 2: Merge per company\n",
    "for company, file_list in company_groups.items():\n",
    "    merged_df = pd.DataFrame(columns=[\"Code\", \"Name\", \"Page Range\", \"Description\", \"Relevant Pages\"])\n",
    "    errors = []\n",
    "\n",
    "    for file in sorted(file_list):\n",
    "        std_df, error = full_content_preserving_standardize(file)\n",
    "        if error:\n",
    "            errors.append((file, error))\n",
    "            continue\n",
    "        if std_df is not None:\n",
    "            # Override Page Range and Relevant Pages using folder name\n",
    "            folder = Path(file).parent.name\n",
    "            match = re.search(r'page[_\\-]?0*([1-9]\\d{0,3})', folder)\n",
    "            page_str = match.group(1) if match else ''\n",
    "            if page_str:\n",
    "                std_df[\"Page Range\"] = page_str\n",
    "                std_df[\"Relevant Pages\"] = page_str\n",
    "            merged_df = pd.concat([merged_df, std_df], ignore_index=True)\n",
    "\n",
    "    if errors:\n",
    "        company_error_log[company] = errors\n",
    "\n",
    "    if merged_df.empty:\n",
    "        continue\n",
    "\n",
    "    # ✅ Normalize: remove \"ESRS 2 \" from start of code\n",
    "    merged_df[\"Code\"] = merged_df[\"Code\"].str.replace(r'(?i)^ESRS\\s*\\d*\\s*', '', regex=True).str.strip()\n",
    "\n",
    "    # ⚠️ Only group if multiple rows have same Code – otherwise skip grouping for now\n",
    "    merged_df[\"Relevant Pages\"] = merged_df.apply(\n",
    "        lambda r: extract_relevant_pages(r[\"Description\"], r[\"Name\"], r[\"Page Range\"]), axis=1\n",
    "    )\n",
    "    merged_df[\"Relevant Pages\"] = merged_df.apply(clean_relevant_pages_from_range, axis=1)\n",
    "    merged_df[\"Section Reference\"] = merged_df.apply(\n",
    "        lambda r: extract_section_reference(r[\"Name\"], r[\"Description\"]), axis=1\n",
    "    )\n",
    "\n",
    "    # Step: Clean it against page range and standalone number rules\n",
    "    def clean_section_against_pages(row):\n",
    "        section = str(row[\"Section Reference\"]).strip()\n",
    "        if not section:\n",
    "            return \"\"\n",
    "\n",
    "        # Extract all page numbers from Page Range and Relevant Pages\n",
    "        page_vals = set(re.findall(r'\\d{2,4}', str(row[\"Page Range\"])))\n",
    "        rel_vals = set(re.findall(r'\\d{2,4}', str(row[\"Relevant Pages\"])))\n",
    "        combined_pages = page_vals.union(rel_vals)\n",
    "\n",
    "        # Keep only valid parts\n",
    "        filtered_parts = []\n",
    "        for part in section.split(\" | \"):\n",
    "            clean = part.strip()\n",
    "            if clean.isdigit():\n",
    "                if clean in combined_pages or int(clean) > 1000:\n",
    "                    continue\n",
    "            filtered_parts.append(clean)\n",
    "\n",
    "        return \" | \".join(filtered_parts)\n",
    "\n",
    "    merged_df[\"Section Reference\"] = merged_df.apply(clean_section_against_pages, axis=1)\n",
    "\n",
    "    # ✅ Group but retain first non-empty Page Range\n",
    "    group_dict = {\n",
    "        \"Name\": lambda x: \" | \".join(x.dropna().unique()),\n",
    "        \"Description\": lambda x: \" | \".join(x.dropna().unique()),\n",
    "        \"Page Range\": lambda x: next((v for v in x if pd.notna(v) and str(v).strip()), \"\"),\n",
    "        \"Relevant Pages\": lambda x: \",\".join(sorted(\n",
    "            {p.strip() for p in \",\".join(x.astype(str)).split(\",\") if p.strip()},\n",
    "            key=lambda p: int(p) if p.isdigit() else float('inf')\n",
    "        )),\n",
    "    }\n",
    "\n",
    "    if \"Section Reference\" in merged_df.columns:\n",
    "        group_dict[\"Section Reference\"] = lambda x: next((v for v in x if pd.notna(v) and str(v).strip()), \"\")\n",
    "\n",
    "    merged_df = merged_df.groupby(\"Code\", as_index=False).agg(group_dict)\n",
    "\n",
    "    # ✅ Save result\n",
    "    output_path = merged_output_dir / f\"{company}_standardized_full.csv\"\n",
    "    print(f\"✅ Saved updated: {output_path} (rows: {len(merged_df)})\")\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "    last_merged_df = merged_df.copy()\n",
    "\n",
    "# Step 3: Summary and preview\n",
    "print(\"📂 Output Directory:\", merged_output_dir)\n",
    "print(\"\\n⚠️ Errors (if any):\")\n",
    "for company, errors in list(company_error_log.items())[:3]:\n",
    "    print(f\"- {company}: {len(errors)} error(s)\")\n",
    "\n",
    "print(\"\\n✅ Preview of Last Merged Company Table:\")\n",
    "if last_merged_df is not None:\n",
    "    print(last_merged_df.head(10))\n",
    "else:\n",
    "    print(\"No valid company tables processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82470ff-6a27-4167-8c4c-1079750744cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4831a2-5637-488e-8cf5-6995a71703d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esg-py311)",
   "language": "python",
   "name": "esg-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
