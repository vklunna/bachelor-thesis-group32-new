{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ace09abe-a0f3-4974-817d-b21872899e60",
   "metadata": {},
   "source": [
    "Prepocess text: lowercasing, removing punctuation and stopwords (excluding may, will, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298e35ff-0398-48ae-a429-4fb4570a25aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /Users/valeriiaklynna/anaconda3/envs/esg-py311/lib/python3.11/site-packages (from nltk) (8.2.1)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/valeriiaklynna/anaconda3/envs/esg-py311/lib/python3.11/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_10_9_x86_64.whl (287 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [nltk][32m2/3\u001b[0m [nltk]b]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.1 nltk-3.9.1 regex-2024.11.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d3d9f3-c546-404d-905c-d01963b75303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/valeriiaklynna/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/valeriiaklynna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download resources once\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# Define the modals to preserve\n",
    "MODAL_VERBS = {\n",
    "    \"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"\n",
    "}\n",
    "\n",
    "# Build custom stopword list (remove modal verbs from standard stopwords)\n",
    "stop_words = set(stopwords.words(\"english\")) - MODAL_VERBS\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    # Keep standard ASCII characters and typical punctuation\n",
    "    return re.sub(r\"[^\\x00-\\x7F]+\", \"\", text)\n",
    "\n",
    "def preprocess_text(raw_text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by:\n",
    "    - Removing non-ASCII characters\n",
    "    - Lowercasing\n",
    "    - Removing punctuation (but preserving sentence structure)\n",
    "    - Removing stopwords (except modal verbs)\n",
    "    Returns a list of cleaned sentences.\n",
    "    \"\"\"\n",
    "    preprocessed_sentences = []\n",
    "\n",
    "    # Clean unusual special characters\n",
    "    raw_text = remove_special_chars(raw_text)\n",
    "\n",
    "    # Sentence tokenize\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Lowercase\n",
    "        sentence = sentence.lower()\n",
    "\n",
    "        # Remove basic punctuation (keep alphanumeric and whitespace)\n",
    "        sentence = re.sub(r\"[^a-z0-9\\s]\", \"\", sentence)\n",
    "\n",
    "        # Tokenize and filter stopwords\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [w for w in words if w not in stop_words]\n",
    "\n",
    "        # Reconstruct cleaned sentence\n",
    "        cleaned_sentence = \" \".join(filtered_words)\n",
    "        preprocessed_sentences.append(cleaned_sentence)\n",
    "\n",
    "    return preprocessed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "614e311f-ded7-41fd-850c-3852ae318dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed: High_Emission.txt → High_Emission_preprocessed.txt\n",
      "✅ Preprocessed: Low_Emission.txt → Low_Emission_preprocessed.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Path setup\n",
    "repo_root = Path().resolve().parent\n",
    "input_folder = repo_root / \"2_output\" / \"extracted_text_indiv\"\n",
    "output_folder = repo_root / \"2_output\" / \"extracted_text_indiv\"\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for filename in [\"High_Emission.txt\", \"Low_Emission.txt\"]:\n",
    "    input_path = input_folder / filename\n",
    "    output_path = output_folder / filename.replace(\".txt\", \"_preprocessed.txt\")\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    preprocessed_sentences = preprocess_text(raw_text)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        out_f.write(\"\\n\".join(preprocessed_sentences))\n",
    "\n",
    "    print(f\"✅ Preprocessed: {filename} → {output_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be2029b-6cd7-4b33-b2ae-926c12bdb049",
   "metadata": {},
   "source": [
    "Hedging anf forward-looking trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "826e7530-fab3-4729-b790-c358424dbd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hedging and forward-looking phrases\n",
    "HEDGING_PHRASES = [\n",
    "    \"may\", \"might\", \"could\", \"suggest\", \"possibly\", \"potentially\", \"assume\",\n",
    "    \"expected to\", \"anticipated\", \"planned\", \"typically\", \"generally\", \"likely\",\n",
    "    \"intend\", \"estimate\", \"believe\", \"aim\", \"tend to\"\n",
    "]\n",
    "\n",
    "# Forward-looking indicators (future promises/plans)\n",
    "FORWARD_LOOKING_PHRASES = [\n",
    "    \"will\", \"plan to\", \"aim to\", \"target\", \"commit to\", \"intend to\",\n",
    "    \"expect\", \"expected to\", \"will reduce\", \"future\", \"in the coming years\", \"forecast\", \"projection\"\n",
    "]\n",
    "\n",
    "# Stopwords minus modal verbs\n",
    "MODALS = {\"can\", \"could\", \"may\", \"might\", \"must\", \"shall\", \"should\", \"will\", \"would\"}\n",
    "stop_words = set(stopwords.words(\"english\")) - MODALS\n",
    "\n",
    "# === Functions ===\n",
    "def count_phrases(text, phrases):\n",
    "    \"\"\"\n",
    "    Count occurrences of each phrase in the text (case-insensitive).\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    for phrase in phrases:\n",
    "        # Match whole words or exact phrases, ignoring case\n",
    "        pattern = re.compile(rf\"\\b{re.escape(phrase)}\\b\", flags=re.IGNORECASE)\n",
    "        matches = pattern.findall(text)\n",
    "        if matches:\n",
    "            counts[phrase] = len(matches)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d42764-f538-4fe2-a238-071d6c2a94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "high_path = Path(\"../2_output/extracted_text_indiv/High_Emission_preprocessed.txt\")\n",
    "low_path = Path(\"../2_output/extracted_text_indiv/Low_Emission_preprocessed.txt\")\n",
    "\n",
    "with open(high_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    high_text = f.read()\n",
    "\n",
    "with open(low_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    low_text = f.read()\n",
    "\n",
    "# Count hedging and forward-looking\n",
    "high_hedging = count_phrases(high_text, HEDGING_PHRASES)\n",
    "low_hedging = count_phrases(low_text, HEDGING_PHRASES)\n",
    "\n",
    "high_forward = count_phrases(high_text, FORWARD_LOOKING_PHRASES)\n",
    "low_forward = count_phrases(low_text, FORWARD_LOOKING_PHRASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003bd9fc-d6cd-4695-b446-7a5ad7b7ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hedging Phrase Frequency:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>may</th>\n",
       "      <th>might</th>\n",
       "      <th>could</th>\n",
       "      <th>suggest</th>\n",
       "      <th>possibly</th>\n",
       "      <th>potentially</th>\n",
       "      <th>assume</th>\n",
       "      <th>anticipated</th>\n",
       "      <th>planned</th>\n",
       "      <th>typically</th>\n",
       "      <th>generally</th>\n",
       "      <th>likely</th>\n",
       "      <th>intend</th>\n",
       "      <th>estimate</th>\n",
       "      <th>believe</th>\n",
       "      <th>aim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>477</td>\n",
       "      <td>37</td>\n",
       "      <td>375</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>151</td>\n",
       "      <td>15</td>\n",
       "      <td>83</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>385</td>\n",
       "      <td>47</td>\n",
       "      <td>215</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>116</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      may  might  could  suggest  possibly  potentially  assume  anticipated  \\\n",
       "High  477     37    375        1         1           71       3           96   \n",
       "Low   385     47    215        1         5           79      16           32   \n",
       "\n",
       "      planned  typically  generally  likely  intend  estimate  believe  aim  \n",
       "High      151         15         83      47      12        56       52  181  \n",
       "Low       116         27         27      93      13        38       24  207  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Forward-Looking Phrase Frequency:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>will</th>\n",
       "      <th>target</th>\n",
       "      <th>expect</th>\n",
       "      <th>will reduce</th>\n",
       "      <th>future</th>\n",
       "      <th>forecast</th>\n",
       "      <th>projection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>866</td>\n",
       "      <td>1492</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>420</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>869</td>\n",
       "      <td>1006</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>257</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      will  target  expect  will reduce  future  forecast  projection\n",
       "High   866    1492      27           14     420        10           7\n",
       "Low    869    1006      63           16     257        19           6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Combine results\n",
    "hedging_df = pd.DataFrame([high_hedging, low_hedging], index=[\"High\", \"Low\"]).fillna(0).astype(int)\n",
    "forward_df = pd.DataFrame([high_forward, low_forward], index=[\"High\", \"Low\"]).fillna(0).astype(int)\n",
    "\n",
    "# Display in notebook\n",
    "print(\" Hedging Phrase Frequency:\")\n",
    "display(hedging_df)\n",
    "\n",
    "print(\"\\n Forward-Looking Phrase Frequency:\")\n",
    "display(forward_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce95e01e-b9bc-4d9a-8c56-bbfe483c3eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Top Hedging Phrases (High Emission):\n",
      "[('may', 477), ('could', 375), ('aim', 181), ('planned', 151), ('anticipated', 96), ('generally', 83), ('potentially', 71), ('estimate', 56), ('believe', 52), ('likely', 47)]\n",
      "\n",
      "🔎 Top Hedging Phrases (Low Emission):\n",
      "[('may', 385), ('could', 215), ('aim', 207), ('planned', 116), ('likely', 93), ('potentially', 79), ('might', 47), ('estimate', 38), ('anticipated', 32), ('typically', 27)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Reconstruct counters\n",
    "high_counts_hedging = Counter(high_hedging)\n",
    "low_counts_hedging = Counter(low_hedging)\n",
    "\n",
    "# Show results\n",
    "print(\"🔎 Top Hedging Phrases (High Emission):\")\n",
    "print(high_counts_hedging.most_common(10))\n",
    "\n",
    "print(\"\\n🔎 Top Hedging Phrases (Low Emission):\")\n",
    "print(low_counts_hedging.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3334316-4244-4c1a-b489-c1b273996531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 Top Forwand Looking (High Emission):\n",
      "[('target', 1492), ('will', 866), ('future', 420), ('expect', 27), ('will reduce', 14), ('forecast', 10), ('projection', 7)]\n",
      "\n",
      "🔎 Top Forward Looking (Low Emission):\n",
      "[('target', 1006), ('will', 869), ('future', 257), ('expect', 63), ('forecast', 19), ('will reduce', 16), ('projection', 6)]\n"
     ]
    }
   ],
   "source": [
    "high_counts_forward = Counter(high_forward)\n",
    "low_counts_forward = Counter(low_forward)\n",
    "\n",
    "print(\"🔎 Top Forwand Looking (High Emission):\")\n",
    "print(high_counts_forward.most_common(10))\n",
    "\n",
    "print(\"\\n🔎 Top Forward Looking (Low Emission):\")\n",
    "print(low_counts_forward.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dca95d-8456-4bd2-a0ee-0e3840ff7f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (esg-py311)",
   "language": "python",
   "name": "esg-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
